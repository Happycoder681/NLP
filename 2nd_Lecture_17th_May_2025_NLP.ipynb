{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# word tokenization\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed\n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\""
      ],
      "metadata": {
        "id": "3qX-SNM_cRDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text.split())  #\"all the words are sep on the basis of spaces\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyS2mmWwcrt6",
        "outputId": "01ff2475-8ac5-4772-d8ed-a2c12982340f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Founded', 'in', '2002,', 'SpaceX’s', 'mission', 'is', 'to', 'enable', 'humans', 'to', 'become', 'a', 'spacefaring', 'civilization', 'and', 'a', 'multi-planet', 'species', 'by', 'building', 'a', 'self-sustaining', 'city', 'on', 'Mars.', 'In', '2008,', 'SpaceX’s', 'Falcon', '1', 'became', 'the', 'first', 'privately', 'developed', 'liquid-fuel', 'launch', 'vehicle', 'to', 'orbit', 'the', 'Earth.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text.split(\",\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbEBzAykczt6",
        "outputId": "726e1bfa-7839-4a41-ed39-4c7215eb307b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Founded in 2002', ' SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\\nspecies by building a self-sustaining city on Mars. In 2008', ' SpaceX’s Falcon 1 became the first privately developed\\nliquid-fuel launch vehicle to orbit the Earth.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text.split(\".\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpwfMMckdhGK",
        "outputId": "2bd95c44-9038-46aa-dc3c-3ef4127247cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\\nspecies by building a self-sustaining city on Mars', ' In 2008, SpaceX’s Falcon 1 became the first privately developed\\nliquid-fuel launch vehicle to orbit the Earth', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "qcb9IMBvdzGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = \"[\\w]+\"\n",
        "tokens=re.findall(pattern,text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBWU50sEd7J9",
        "outputId": "3537bb76-6537-4337-c40c-1f7467a20dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Founded', 'in', '2002', 'SpaceX', 's', 'mission', 'is', 'to', 'enable', 'humans', 'to', 'become', 'a', 'spacefaring', 'civilization', 'and', 'a', 'multi', 'planet', 'species', 'by', 'building', 'a', 'self', 'sustaining', 'city', 'on', 'Mars', 'In', '2008', 'SpaceX', 's', 'Falcon', '1', 'became', 'the', 'first', 'privately', 'developed', 'liquid', 'fuel', 'launch', 'vehicle', 'to', 'orbit', 'the', 'Earth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# senetence tokenization\n",
        "sentences=re.split(r'[.!?]+',text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-566yTId7Qb",
        "outputId": "17910c79-9f5e-46dd-d21a-04b2ed34e11e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\\nspecies by building a self-sustaining city on Mars', ' In 2008, SpaceX’s Falcon 1 became the first privately developed\\nliquid-fuel launch vehicle to orbit the Earth', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"This is the first sentence.\n",
        "However, some of the Ph.D. students are working with some\n",
        "researcher under Dr. and Scientists.\"\"\"\n",
        "sentences=re.split(r'[.!?]+',text)\n",
        "print(sentences)  #here the issue is with PhD, usme dot ka alag matlab h"
      ],
      "metadata": {
        "id": "MCk2BCAffZqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e53ad98-b26e-458e-97c4-28034ee8245b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is the first sentence', '\\nHowever, some of the Ph', 'D', ' students are working with some \\nresearcher under Dr', ' and Scientists', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since tokenization is a very crucial part there are certain libraries and tools developed for this task only. So we are not going to use regular expression or any other method instead the two widely used libraries like nltk (natural language toolkit) & spacy will be used."
      ],
      "metadata": {
        "id": "MZBXfH3Vg_e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Nltk**"
      ],
      "metadata": {
        "id": "60G026ShmEqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk library\n",
        "# !pip install --user -U nltk"
      ],
      "metadata": {
        "id": "zKbrlygigb5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk.download('punkt')    # ==>This tokenizer divides a text into a list of sentences by using an unsupervised algorithm\n",
        "                                # to build a model for abbreviation and the words that starts sentences.\n",
        "# nltk.download('wordnet')  # ==> Wordnet is a lexical database of English. It helps in finding conceptual relationship b/w words.\n",
        "# nltk.download('stopwords')# ==> Stopwords can remove common words in English and help in word freqency analysis."
      ],
      "metadata": {
        "id": "KzdaVftCihab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab') # ==> This helps in downloading the punk tokenizer & models. It ensures that the NLTK has necessary data to perform\n",
        "# accurate sentence and word tokenization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZIzxcsPi2Er",
        "outputId": "70d86763-108c-475d-9de7-8bef452c989f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed\n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\""
      ],
      "metadata": {
        "id": "2Pl0IkhhkKtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tokens=word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRiwSeXZkr4b",
        "outputId": "ea01f8a6-13a8-4607-8953-6eaf1162f296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Founded', 'in', '2002', ',', 'SpaceX', '’', 's', 'mission', 'is', 'to', 'enable', 'humans', 'to', 'become', 'a', 'spacefaring', 'civilization', 'and', 'a', 'multi-planet', 'species', 'by', 'building', 'a', 'self-sustaining', 'city', 'on', 'Mars', '.', 'In', '2008', ',', 'SpaceX', '’', 's', 'Falcon', '1', 'became', 'the', 'first', 'privately', 'developed', 'liquid-fuel', 'launch', 'vehicle', 'to', 'orbit', 'the', 'Earth', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "tokens=sent_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1brid5JWk6Db",
        "outputId": "2865d39a-c526-42e5-bac9-6fbdcb3dc788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\\nspecies by building a self-sustaining city on Mars.', 'In 2008, SpaceX’s Falcon 1 became the first privately developed\\nliquid-fuel launch vehicle to orbit the Earth.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"This is the first sentence.\n",
        "However, some of the Ph.D. students are working with some\n",
        "researcher under Dr. and Scientists.\"\"\""
      ],
      "metadata": {
        "id": "KJD_o5g7lZU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "tokens=sent_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K3A_IRVlmR6",
        "outputId": "dda28ab4-4bda-4bac-ba4c-00d4ad29b54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is the first sentence.', 'However, some of the Ph.D. students are working with some \\nresearcher under Dr. and Scientists.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Spacy**"
      ],
      "metadata": {
        "id": "NhTZOZxdl4mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install spacy"
      ],
      "metadata": {
        "id": "netVf6eBl8YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "nlp=English()\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed\n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "a=nlp(text)\n",
        "# print(\"a: \",a)\n",
        "token_list=[]\n",
        "for token in a:\n",
        "  token_list.append(token.text)\n",
        "print(token_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjk0kvOcmAt6",
        "outputId": "de16f1b3-35f8-4748-db75-bfb5999992a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Founded', 'in', '2002', ',', 'SpaceX', '’s', 'mission', 'is', 'to', 'enable', 'humans', 'to', 'become', 'a', 'spacefaring', 'civilization', 'and', 'a', 'multi', '-', 'planet', '\\n', 'species', 'by', 'building', 'a', 'self', '-', 'sustaining', 'city', 'on', 'Mars', '.', 'In', '2008', ',', 'SpaceX', '’s', 'Falcon', '1', 'became', 'the', 'first', 'privately', 'developed', '\\n', 'liquid', '-', 'fuel', 'launch', 'vehicle', 'to', 'orbit', 'the', 'Earth', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp=spacy.blank('en')  # loading a blank model\n",
        "nlp.add_pipe('sentencizer')  # Adding the senetnce parser\n",
        "text =\"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed\n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "a = nlp(text)\n",
        "sent_list=[]\n",
        "for sent in a.sents:\n",
        "  sent_list.append(sent.text)\n",
        "\n",
        "print(sent_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzLw32ckpOQV",
        "outputId": "8ade9a59-160f-445b-c969-c0b5424206c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\\nspecies by building a self-sustaining city on Mars.', 'In 2008, SpaceX’s Falcon 1 became the first privately developed\\nliquid-fuel launch vehicle to orbit the Earth.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp=spacy.blank('en')  # loading a blank model\n",
        "nlp.add_pipe('sentencizer')  # Adding the senetnce parser\n",
        "text = \"\"\"This is the first sentence.\n",
        "However, some of the Ph.D. students are working with some\n",
        "researcher under Dr. and Scientists.\"\"\"\n",
        "a = nlp(text)\n",
        "sent_list=[]\n",
        "for sent in a.sents:\n",
        "  sent_list.append(sent.text)\n",
        "\n",
        "print(sent_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7TA39JZm2rr",
        "outputId": "6e737e36-33a1-4905-bd91-1cc545b95eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is the first sentence.', '\\nHowever, some of the Ph.D. students are working with some \\nresearcher under Dr. and Scientists.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **tokenization with Keras**"
      ],
      "metadata": {
        "id": "YE3eR6_eqMB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install keras\n",
        "# !pip install tensorflow"
      ],
      "metadata": {
        "id": "vTVGgBMMnrpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet\n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed\n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "result = text_to_word_sequence(text)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXP0ZDcMnrtS",
        "outputId": "0a67c610-260c-4503-bb11-6f17b3c93d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['founded', 'in', '2002', 'spacex’s', 'mission', 'is', 'to', 'enable', 'humans', 'to', 'become', 'a', 'spacefaring', 'civilization', 'and', 'a', 'multi', 'planet', 'species', 'by', 'building', 'a', 'self', 'sustaining', 'city', 'on', 'mars', 'in', '2008', 'spacex’s', 'falcon', '1', 'became', 'the', 'first', 'privately', 'developed', 'liquid', 'fuel', 'launch', 'vehicle', 'to', 'orbit', 'the', 'earth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The advantage of keras is it converts all the words into lower case letter so APPLE apple both will be same and so their number mapping will be same. Keras does not have sentence_tokenizer."
      ],
      "metadata": {
        "id": "M5SyqL4MrAK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stopwords Removal**\n",
        "Stopwords are the words in any language which does not add much meaning to a sentence and can be ignored without sacrificing the meaning of the entire sentence. The most common stopwords in a language are is, a the, etc. In nltk library we can import stopwords from nltk corpus where the stopwords are predefined."
      ],
      "metadata": {
        "id": "ei77EC5s1N-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "Wq_GjQfpnr3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(stopwords.words('english')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZDg_cOY1Jxt",
        "outputId": "1f877e57-5a38-4cfe-99a1-842a1e759f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'being', 'we', 'of', \"she'd\", 'did', \"they'd\", 'were', \"doesn't\", 'ma', 'out', 'a', 'themselves', 'on', \"they've\", 'above', 'yourself', 'be', \"she's\", 'when', \"it'd\", \"that'll\", \"shouldn't\", 'mightn', 'won', 'aren', 'couldn', 'itself', 'during', \"we'd\", 'more', 'while', 'mustn', 'such', 'don', 'does', 'if', 'yours', \"i'm\", 'to', 'why', 'wouldn', 't', 'this', \"hadn't\", 'doing', 'didn', \"needn't\", \"should've\", \"you're\", \"he's\", 'just', 'over', \"i'll\", 'once', \"aren't\", 'any', 'here', 'off', 'same', 'needn', 'been', 'd', 'having', 'they', 's', 'but', \"they'll\", 'weren', 'as', 'ours', 'the', \"we'll\", \"couldn't\", \"you'd\", 'so', 'ain', \"haven't\", 'not', 'now', 'some', \"hasn't\", 'each', \"she'll\", 'all', \"he'd\", 'few', 'y', \"weren't\", 'your', \"don't\", 'down', \"you'll\", 'than', 'these', \"wasn't\", 'in', \"it'll\", 'that', 'have', 've', 'further', \"we've\", \"it's\", 'm', 'from', \"he'll\", 'because', 'nor', 'below', 'with', 'whom', 'can', 'before', \"mustn't\", 'under', 'my', 'myself', 'own', 'through', 'its', 'again', 'hadn', 'i', \"i'd\", 'me', 'her', 'yourselves', 'his', \"wouldn't\", 'for', \"won't\", 'has', 'them', 'which', \"isn't\", 'after', 'll', 'very', 'into', 'she', 're', 'shouldn', 'then', \"we're\", 'most', 'hers', \"i've\", 'both', 'no', 'too', 'where', 'our', 'who', 'how', 'you', 'their', 'theirs', \"they're\", 'isn', 'am', 'doesn', 'are', 'only', 'against', 'should', 'o', 'and', 'at', 'himself', 'herself', 'haven', 'there', \"mightn't\", 'about', 'him', \"you've\", 'up', \"shan't\", 'do', 'until', 'is', 'will', 'an', \"didn't\", 'or', 'other', 'he', 'wasn', 'between', 'what', 'by', 'was', 'shan', 'those', 'had', 'ourselves', 'it', 'hasn'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "text2 = \"\"\"This is the first sentence.\n",
        "However, some of the Ph.D. students are working with some\n",
        "researcher under Dr. and Scientists.\"\"\"\n",
        "stopwords=set(stopwords.words('english'))\n",
        "tokens=word_tokenize(text2)\n",
        "filtered_sentence = []\n",
        "for w in tokens:\n",
        "  if w not in stopwords:\n",
        "    filtered_sentence.append(w)\n",
        "print(' '.join(filtered_sentence))\n",
        "print(\"\\n------------------------\\n\")\n",
        "print(text2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jhAvkoT1J-D",
        "outputId": "10486772-4ac1-4205-fff5-319f43034ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This first sentence . However , Ph.D. students working researcher Dr. Scientists .\n",
            "\n",
            "------------------------\n",
            "\n",
            "This is the first sentence.\n",
            "However, some of the Ph.D. students are working with some \n",
            "researcher under Dr. and Scientists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spacy stopwords removal**"
      ],
      "metadata": {
        "id": "8U10evQg4Pk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English\n",
        "nlp=English()\n",
        "text = \"\"\"This is the first sentence.\n",
        "However, some of the Ph.D. students are working with some\n",
        "researcher under Dr. and Scientists.\"\"\"\n",
        "my_doc = nlp(text)\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "  token_list.append(token.text) # tokenization\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "filtered_sentence = []\n",
        "for word in token_list:\n",
        "  lexeme=nlp.vocab[word]\n",
        "  if lexeme.is_stop==False:\n",
        "    filtered_sentence.append(word)"
      ],
      "metadata": {
        "id": "Ak6XKHoX3ODq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(''.join(filtered_sentence))\n",
        "print(\"\\n------------------------\\n\")\n",
        "print(text2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4o28JEMf4vPb",
        "outputId": "4a2bfd7e-a964-4534-c30d-d862d9bf6b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence.\n",
            ",Ph.D.studentsworking\n",
            "researcherDr.Scientists.\n",
            "\n",
            "------------------------\n",
            "\n",
            "This is the first sentence.\n",
            "However, some of the Ph.D. students are working with some \n",
            "researcher under Dr. and Scientists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords are present very frequently in a sentence. The model thinks they are important instead they are just noise"
      ],
      "metadata": {
        "id": "Kb-rfBTd7BBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **tokenization ke baad hota h punctuation removal**"
      ],
      "metadata": {
        "id": "4UArPNpq7UE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "punc = list(punctuation)\n",
        "print(punc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5j0pfEV7Wnp",
        "outputId": "522f62cb-8c64-4552-b4b1-8f4a8efaf0f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text2 = \"\"\"This is the first sentence.\n",
        "However, some of the Ph.D. students are working with some\n",
        "researcher under Dr.s and Scientists.\"\"\"\n",
        "tokens=word_tokenize(text2)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Nq6ifXn7hCK",
        "outputId": "c85f158f-7504-49e8-cea8-ba07a9a6d2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'the', 'first', 'sentence', '.', 'However', ',', 'some', 'of', 'the', 'Ph.D.', 'students', 'are', 'working', 'with', 'some', 'researcher', 'under', 'Dr.s', 'and', 'Scientists', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "without_punc = [w for w in tokens if w not in punc]\n",
        "print(without_punc)\n",
        "print(\"\\n\")\n",
        "print(\" \".join(without_punc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b03BZ9WN77SK",
        "outputId": "d43aefaf-523c-4171-96bf-412e851fc6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'the', 'first', 'sentence', 'However', 'some', 'of', 'the', 'Ph.D.', 'students', 'are', 'working', 'with', 'some', 'researcher', 'under', 'Dr.s', 'and', 'Scientists']\n",
            "\n",
            "\n",
            "This is the first sentence However some of the Ph.D. students are working with some researcher under Dr.s and Scientists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English\n",
        "nlp=English()\n",
        "text = \"\"\"This is the first sentence.\n",
        "However, some of the Ph.D. students are working with some\n",
        "researcher under Dr. and Scientists.\"\"\"\n",
        "my_doc = nlp(text)\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "  token_list.append(token.text) # tokenization\n",
        "\n",
        "without_punc = [w for w in tokens if w not in punc]\n",
        "print(without_punc)\n",
        "print()\n",
        "print(\" \".join(without_punc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzxBbcOS8rFK",
        "outputId": "2fb77ea6-e7c6-4953-8ecd-f21cdafb0570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'the', 'first', 'sentence', 'However', 'some', 'of', 'the', 'Ph.D.', 'students', 'are', 'working', 'with', 'some', 'researcher', 'under', 'Dr.s', 'and', 'Scientists']\n",
            "\n",
            "This is the first sentence However some of the Ph.D. students are working with some researcher under Dr.s and Scientists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r1X29UyA83Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Normalisation**\n",
        "Text Normalisation is done by stemming and lemmatization\n",
        "1. Stemming - Stemming is the process of reducing a word to its stem called as lema but its drawback is it produces the root word which may not always have meaning. We do stemming or lemmatization because same meaning words should be assigned same number so that machine also understands them to be same. Stemming is only available in nltk not in spacy."
      ],
      "metadata": {
        "id": "VEFTcadc-Lkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "text = \"\"\"This is the first sentence.\n",
        "However, some of the Ph.D. students are working with some\n",
        "researcher under Dr. and Scientists.\"\"\"\n",
        "stopwords = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(text2)\n",
        "filtered_sentence = []\n",
        "for w in tokens:\n",
        "  if w not in stopwords:\n",
        "    filtered_sentence.append(w)\n",
        "without_punc=[w for w in tokens if w not in punc]\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_sentence=[]\n",
        "for w in without_punc:\n",
        "  stemmed_sentence.append(stemmer.stem(w))\n",
        "print(tokens)\n",
        "print(stemmed_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEUc87Rs83Yu",
        "outputId": "adc62df7-dd42-48c9-d022-ed1d541d8803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'the', 'first', 'sentence', '.', 'However', ',', 'some', 'of', 'the', 'Ph.D.', 'students', 'are', 'working', 'with', 'some', 'researcher', 'under', 'Dr.s', 'and', 'Scientists', '.']\n",
            "['thi', 'is', 'the', 'first', 'sentenc', 'howev', 'some', 'of', 'the', 'ph.d.', 'student', 'are', 'work', 'with', 'some', 'research', 'under', 'dr.', 'and', 'scientist']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization**\n",
        "Lemmatization is a process that reduces words to their root form or lemma. Therefore it gives more meaningful words again it depends on the use case wheather to use stemming or lemmatization.\n"
      ],
      "metadata": {
        "id": "8uEXbhxqBvl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "text2 = \"\"\"This is the first sentence.\n",
        "However, some of the Ph.D. students are working with some\n",
        "researcher under Dr. and Scientists.\"\"\"\n",
        "stopwords = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(text2)\n",
        "filtered_sentence = []\n",
        "for w in tokens:\n",
        "  if w not in stopwords:\n",
        "    filtered_sentence.append(w)\n",
        "without_punc=[w for w in filtered_sentence if w not in punc]\n",
        "lemma_word=[]\n",
        "word_net = WordNetLemmatizer()\n",
        "for w in without_punc:\n",
        "  lemma_word.append(word_net.lemmatize(w))\n",
        "print(tokens)\n",
        "print(lemma_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFgwUzhm83b4",
        "outputId": "1b2ed247-f782-4479-a462-d5c8f6289edb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'the', 'first', 'sentence', '.', 'However', ',', 'some', 'of', 'the', 'Ph.D.', 'students', 'are', 'working', 'with', 'some', 'researcher', 'under', 'Dr.', 'and', 'Scientists', '.']\n",
            "['This', 'first', 'sentence', 'However', 'Ph.D.', 'student', 'working', 'researcher', 'Dr.', 'Scientists']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=['study','studying','studied']\n",
        "word_net = WordNetLemmatizer()\n",
        "lemma_verb = [word_net.lemmatize(w,pos=\"v\") for w in words]\n",
        "lemma_noun = [word_net.lemmatize(w,pos=\"n\") for w in words]\n",
        "lemma_adj = [word_net.lemmatize(w,pos=\"a\") for w in words]\n",
        "print(lemma_verb)\n",
        "print(lemma_noun)\n",
        "print(lemma_adj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "253HAwTvD86r",
        "outputId": "5faf5377-99bb-491b-ed1e-4d1044037c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['study', 'study', 'study']\n",
            "['study', 'studying', 'studied']\n",
            "['study', 'studying', 'studied']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=['running','geese','better']\n",
        "word_net = WordNetLemmatizer()\n",
        "lemma_verb = [word_net.lemmatize(w,pos=\"v\") for w in words]\n",
        "lemma_noun = [word_net.lemmatize(w,pos=\"n\") for w in words]\n",
        "lemma_adj = [word_net.lemmatize(w,pos=\"a\") for w in words]\n",
        "print(lemma_verb)\n",
        "print(lemma_noun)\n",
        "print(lemma_adj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3t1cVi_EBpv",
        "outputId": "6f0c647f-261b-4e53-bcfc-83d85a644145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'geese', 'better']\n",
            "['running', 'goose', 'better']\n",
            "['running', 'geese', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ala9N8X6EBwP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}